{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Load required libraries"
      ],
      "metadata": {
        "id": "ahMP1n5Dt1nL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftqgzJgXrX-r"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers peft accelerate optimum\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/\n",
        "!pip install -q datasets\n",
        "!pip install loralib==0.1.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
        "import torch\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling"
      ],
      "metadata": {
        "id": "U-O9Yp5bta35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load quantized model"
      ],
      "metadata": {
        "id": "LM79-uqUuD0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the model ID to be loaded:\n",
        "\n",
        "model_id = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
        "\n",
        "# Define the quantization configuration:\n",
        "gptq_config  = GPTQConfig(bits=4  # Quantize model weights to 4 bits for reduced size and faster inference.\n",
        "                          , disable_exllama=True) # disabled the exllama kernel because training with exllama kernel is unstable\n",
        "\n",
        "# Load the quantized model:\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config=gptq_config ,\n",
        "                                             device_map=\"auto\", # Automatically distribute the model across available devices (if applicable)\n",
        "                                             trust_remote_code=True) # Necessary for loading models with custom code components."
      ],
      "metadata": {
        "id": "oblVqlfBuPRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the appropriate tokenizer for the specified model:\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "NsYUpgn20GQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.quantization_config.to_dict()"
      ],
      "metadata": {
        "id": "-QjadqPHur6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the dataset"
      ],
      "metadata": {
        "id": "S977V73Iv4yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"ttbui/alpaca_webgen_html\", split=\"train\")\n",
        "data"
      ],
      "metadata": {
        "id": "YAsKTAxxv6oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(dataset):\n",
        "  #Data Structure Check:\n",
        "    if \"instruction\" in dataset and \"output\" in dataset:\n",
        "    #Prompt Construction:\n",
        "      prompt_template = \"Below is instruction that describes a task to code in HTML,what is output in HTML: \\n \\n'\"\n",
        "      instruction = dataset[\"instruction\"][0]\n",
        "      response = dataset[\"output\"][0]\n",
        "\n",
        "      text_with_prompt = (prompt_template +\n",
        "                          '### Instruction: \\n' +instruction +\n",
        "                          '\\n ### Response: \\n' + response)\n",
        "\n",
        "    #Tokenization\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Set padding token to the end-of-sentence token\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text_with_prompt,\n",
        "        return_tensors=\"np\",   #Return NumPy tensors\n",
        "        padding=True,    #Pad sequences to equal length\n",
        "    )\n",
        "\n",
        "    max_length = min(\n",
        "        tokenized_inputs[\"input_ids\"].shape[1],\n",
        "        2048    # Set maximum length to 2048 or the actual length, whichever is shorter\n",
        "    )\n",
        "    tokenizer.truncation_side = \"left\"  # Truncate from the left if necessary\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text_with_prompt,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,   # Enable truncation\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "jujOeEDIwhRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization Mapping\n",
        "tokenized_dataset = data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    drop_last_batch=True\n",
        ")"
      ],
      "metadata": {
        "id": "5uVSF-WH0DFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "1vnHtkJl1abB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting into Testing and training sets\n",
        "data_split = tokenized_dataset.train_test_split(test_size=0.25, shuffle=True, seed=123)\n",
        "data_split"
      ],
      "metadata": {
        "id": "5dT8Ogi11hUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check base model results"
      ],
      "metadata": {
        "id": "Ch3sVhTuFTxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_exact_match(a, b):\n",
        "    return a.strip() == b.strip()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  input_ids = tokenizer.encode(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "eKXPmtfeFRxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a specific test question from the dataset:\n",
        "test_question = data_split[\"test\"]['instruction'][2]\n",
        "\n",
        "# Generate an answer using the model and tokenizer:\n",
        "generated_answer = inference(test_question, model, tokenizer)\n",
        "\n",
        "print(test_question)\n",
        "print(generated_answer)"
      ],
      "metadata": {
        "id": "_F9VUp0EFqzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the fine-tuned model from local"
      ],
      "metadata": {
        "id": "E_F_EzVx4SjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content/output_dirc\""
      ],
      "metadata": {
        "id": "zsEuZZC94U8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gptq_config = GPTQConfig(bits=4, use_exllama=False)\n",
        "\n",
        "trained_model = AutoModelForCausalLM.from_pretrained(\n",
        "output_dir, local_files_only=True,\n",
        "quantization_config=gptq_config,\n",
        "trust_remote_code=True, device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "Be5A_SnY5A-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate the model"
      ],
      "metadata": {
        "id": "C5Y_unjxA5ZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run model and compare to expected answer"
      ],
      "metadata": {
        "id": "40kJ2vwKJCHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = data_split[\"test\"]['instruction'][2]\n",
        "generated_answer = inference(test_question, trained_model, tokenizer)\n",
        "print(test_question)\n",
        "print(generated_answer)"
      ],
      "metadata": {
        "id": "4BEhkp54BHCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = data_split[\"test\"]['output'][2]\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "AZxxqeXkHIlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exact_match = is_exact_match(generated_answer, answer)\n",
        "print(exact_match)"
      ],
      "metadata": {
        "id": "ILpvW63rHiln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run over entire dataset and compare"
      ],
      "metadata": {
        "id": "OTEFkmF6Hotz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Hn8lAdpXHziX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing Variables:\n",
        "n = 20\n",
        "metrics = {'exact_matches': []}\n",
        "predictions = []\n",
        "\n",
        "#Iterating through Test Data\n",
        "\n",
        "for i, item in tqdm(enumerate(data_split[\"test\"])):\n",
        "    print(\"i Evaluating: \" + str(item))\n",
        "    instruction = item['instruction']\n",
        "    output = item['output']\n",
        "\n",
        " #Generating Predictions\n",
        "    try:\n",
        "      predicted_output = inference(instruction, trained_model, tokenizer)\n",
        "    except:\n",
        "      continue\n",
        "    predictions.append([predicted_output, output])\n",
        "\n",
        "  #Calculating Exact Match Metric\n",
        "    #fixed: exact_match = is_exact_match(generated_output, output)\n",
        "    exact_match = is_exact_match(predicted_output, output)\n",
        "    metrics['exact_matches'].append(exact_match)\n",
        "\n",
        "   #Terminating Early (Optional)\n",
        "    if i > n and n != -1:\n",
        "      break\n",
        "print('Number of exact matches: ', sum(metrics['exact_matches']))"
      ],
      "metadata": {
        "id": "UJ5r6rsuHnG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ZERO!! This metric for evaluation is not useful for this dataset"
      ],
      "metadata": {
        "id": "OPnjoBgVzOrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(predictions, columns=[\"predicted_answer\", \"target_answer\"])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "RyMs-S10JGTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df['predicted_answer'])"
      ],
      "metadata": {
        "id": "h2mLW6iq3iMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df['target_answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_aHbUuV4w3S",
        "outputId": "f5341047-59a7-4a97-e0c1-54a1b06a37b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation with Metric: chr_f"
      ],
      "metadata": {
        "id": "Cur3M43i6Dgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChrF is a evaluation metrics that use the F-score statistic for character n-gram matches. We use the implementation that is already present in sacrebleu"
      ],
      "metadata": {
        "id": "PL7Pbeo39lXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "vBWLY3qF1M7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric  # For sacrebleu CHRF\n",
        "chrf = load_metric(\"chrf\")"
      ],
      "metadata": {
        "id": "3nNNHibZ3DdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = df['predicted_answer'].tolist()  # Convert Series to list\n",
        "reference = df['target_answer'].tolist()  # Convert Series to list\n",
        "\n",
        "# Create a list of lists for reference (if needed)\n",
        "if not isinstance(reference[0], list):\n",
        "    reference = [[ref] for ref in reference]"
      ],
      "metadata": {
        "id": "pnZBGAfl3_EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = chrf.compute(predictions=prediction, references=reference)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "llF_toM85ufx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}